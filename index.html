<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Taha ValizadehAslani</title>
  <meta name="description" content="Personal site for Taha ValizadehAslani" />
  <meta name="color-scheme" content="light dark" />
  <link rel="icon" href="/favicon.ico" />
  <style>
    :root {
      --bg: #ffffff;
      --ink: #151515;
      --ink-2: #545454;
      --rule: #e4e4e4;
      --accent: #1a4dd6;
      --col-rail: 300px;
      --col-gap: 56px;
      --measure: 68ch;
      --leading: 1.55;
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0f1113;
        --ink: #e9eaec;
        --ink-2: #a1a6ad;
        --rule: #2a2e33;
        --accent: #84a7ff;
      }
    }
    html { -webkit-text-size-adjust: 100%; }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      background: var(--bg);
      color: var(--ink);
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
      line-height: var(--leading);
    }
    .wrap { display: grid; grid-template-columns: var(--col-rail) var(--col-gap) 1fr; min-height: 100dvh; }
    .rail { padding: 48px 24px 48px 56px; border-right: 1px solid var(--rule); position: sticky; top: 0; align-self: start; height: 100dvh; }
    .main { padding: 64px 56px 96px; max-width: 1200px; }
    .name { font-family: ui-serif, Georgia, "Times New Roman", serif; font-weight: 700; font-size: 24px; margin: 0 0 4px; }
    .role { color: var(--ink-2); margin: 0 0 28px; font-size: 14px; }
    nav a { display: block; text-decoration: none; color: var(--ink-2); padding: 6px 0; font-weight: 600; }
    nav a:hover, nav a[aria-current="page"] { color: var(--ink); text-decoration: underline; text-underline-offset: 3px; }
    h1, h2, h3 { font-weight: 800; margin: 0; line-height: 1.2; }
    h1 { font-size: 40px; margin-bottom: 12px; }
    h2 { font-size: 24px; margin: 48px 0 12px; border-top: 1px solid var(--rule); padding-top: 24px; }
    h3 { font-size: 18px; margin: 28px 0 6px; }
    p { margin: 0 0 14px; max-width: var(--measure); color: var(--ink); }
    .muted { color: var(--ink-2); }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; text-underline-offset: 2px; }
    ul { padding-left: 18px; margin: 0 0 14px; max-width: var(--measure); }
    .intro { max-width: var(--measure); }
    .meta { display: flex; gap: 14px; color: var(--ink-2); font-size: 14px; }
    .projects { display: grid; grid-template-columns: 1fr; gap: 40px; max-width: 1000px; }
    .project { border-top: 1px solid var(--rule); padding-top: 16px; }
    .project img {
    display: block;
    max-width: 68ch;  /* same as paragraph text width */
    width: 100%;
    height: auto;
    /* border-radius: 50%; */
    margin: 12px auto;
    }
    .contact dd, .contact dt { margin: 0; }
    .contact dt { color: var(--ink-2); font-size: 12px; text-transform: uppercase; letter-spacing: .08em; margin-top: 18px; }
    footer { border-top: 1px solid var(--rule); margin-top: 64px; padding-top: 16px; color: var(--ink-2); font-size: 14px; }
    @media (max-width: 1100px) {
      .wrap { grid-template-columns: 1fr; }
      .rail { position: static; height: auto; border-right: none; border-bottom: 1px solid var(--rule); padding: 28px 24px; }
      .main { padding: 40px 24px 64px; }



    }
  </style>
</head>
<body>
  <div class="wrap">
    <aside class="rail">
      <h1 class="name">Taha ValizadehAslani</h1>
      <p class="role">Machine Learning Engineer <br> Philadelphia, PA</p>
<!--       <p class="role">Philadelphia, USA</p> -->
      <nav>
        <a href="#about" aria-current="page">About</a>
        <a href="#work">Selected Work</a>
        <a href="#Experience">Experience</a>
        <a href="#Education">Education</a>
        <a href="#Publications">Publications</a>
        <a href="#books">Book Recommendations</a>
        <a href="#contact">Contact</a>
        <a href="https://raw.githubusercontent.com/TahaAslani/tahaaslani.github.io/main/Resume_TV.pdf">Résumé</a>
      </nav>
    </aside>
    <div></div>
    <main class="main">
      <h1>Machine Learning and More</h1>
      <section id="about" class="intro">
        <h2>About Me</h2>
        <img src="images/Taha.png" alt="Taha ValizadehAslani" style="max-width:40%; height:auto; margin:8px 0; border-radius: 50%;">
        <p class="muted">I work on <strong>machine learning</strong>, <strong>natural language processing</strong>, <strong>data science</strong>,  and <strong>information theory</strong>. I care about models that not only get the job done but are also theoretically proven to be optimal.<br>
          Personally, I am interested in learning about psychology, social sciences, and history.</p>
        <div class="meta" style="margin-top:8px">
          <span><a href="https://raw.githubusercontent.com/TahaAslani/tahaaslani.github.io/main/Resume_TV.pdf">Résumé</a></span>
          <span>•</span>
          <span><a href="mailto:taha.valizadeh@gmail.com">Email</a></span>
          <span>•</span>
          <span><a href="https://scholar.google.com/citations?user=WxaHx0sAAAAJ&hl=en">Google Scholar</a></span>
          <span>•</span>
          <span><a href="https://github.com/TahaAslani" target="_blank" rel="noreferrer noopener">GitHub</a></span>
          <span>•</span>
          <span><a href="https://www.linkedin.com/in/taha-valizadehaslani-699538124" target="_blank" rel="noreferrer noopener">LinkedIn</a></span>
        </div>
      </section>


      

      
      <section id="work">
        <h2>Selected Work</h2>
        <div class="projects">

          <article class="project">
            <h3>
              <a href="https://academic.oup.com/bib/article/24/4/bbad226/7197744" target="_blank" rel="noreferrer noopener">
                PharmBERT: An LLM for pharmaceutical texts
              </a>
            </h3>
            <p class="muted">Drug labels aren’t like ordinary text. They’re packed with highly specialized language, strict regulatory phrasing, and complex medical terminology. This makes them challenging for generic large language models (LLMs), which lack the domain knowledge needed to interpret pharmaceutical text with accuracy and reliability.<br>That’s where PharmBERT comes in. Built specifically for the U.S. Food and Drug Administration (FDA), PharmBERT is a first-of-its-kind, purpose-built model designed to understand the unique world of drug labels. Unlike general-purpose LLMs, PharmBERT has been trained on a vast range of pharmaceutical texts, giving it the expertise to decode complex compound names, regulatory language, and medical terminology with unmatched precision.</p>
            <img src="images/PharmBERT.png" alt="PharmBERT concept art illustrating domain-specific pretraining for drug labels" style="max-width:60%; height:auto; margin:8px 0;">
            <p class="muted">PharmBERT bridges the gap between powerful language processing and the strict demands of the pharmaceutical industry. The result: more accurate insights, greater reliability, and contextually relevant analysis that general models simply can’t match.<br>With PharmBERT, pharmaceutical companies, healthcare professionals, and regulators gain a specialized tool that transforms drug-label text into actionable knowledge—delivering clarity, confidence, and compliance at every step.</p>
          </article>

          <article class="project">
              <h3>
                <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7694136/" target="_blank" rel="noreferrer noopener">
                  Amino Acid k-mer Features for Antimicrobial Resistance Prediction
                </a>
              </h3>
            <p class="muted">Microbes can adapt over time, developing resistance to antibiotics—a phenomenon known as antimicrobial resistance (AMR). Once this resistance emerges, treating infections becomes much more difficult, often requiring stronger or alternative antibiotics, many of which may carry greater risks. Misuse and overuse of antibiotics have accelerated this process, putting constant evolutionary pressure on microbes and driving the rise of AMR. Today, this issue is widely regarded as one of the most serious public-health threats, with the potential to trigger crises comparable to pandemics if effective treatments are unavailable.</p>
            <p class="muted">The aim of this project was to design and test machine learning approaches that leverage bacterial genome sequences in two ways: first, to estimate the minimum antibiotic dosage needed to combat an infection; and second, to pinpoint mutations or genes that contribute to resistance. I introduced a new feature-extraction method—counting amino-acid k-mers—that enables machine-learning models to recognize recurring patterns in amino acid sequences within bacterial genes. I compared the proposed method to the existing methods, such as counting nucleotide (NT) k-mers, gene searching, and single-nucleotide polymorphism (SNP) calling. The proposed approach not only matches or surpasses the accuracy of current techniques for predicting dosage but also offers researchers deeper insight into how resistance arises at the molecular level.</p>
            <img src="images/AMR.png" alt="Visualization of implemented methods for analyzing bacterial genomes" style="max-width:60%; height:auto; margin:8px 0;">
          </article>
          
          <article class="project">
            <h3>
              <a href="https://www.sciencedirect.com/science/article/pii/S0925231224005721" target="_blank" rel="noreferrer noopener">
                Two-stage fine-tuning for learning class-imbalanced data
              </a>
            </h3>  
            <p class="muted">In practical classification scenarios, it's common to encounter an imbalanced or long-tailed distribution of classes. This situation arises when certain classes within the dataset, known as minority classes, are represented by a relatively small number of samples, while others, referred to as majority classes, are characterized by a significantly larger number of samples. This non-uniform distribution of samples among classes presents a challenge to a wide array of machine learning algorithms. This issue is further exacerbated if the selected performance metric values all classes equally, irrespective of their frequency in the dataset, for reasons such as fairness and inclusivity or because of an intrinsic interest in the minority class.</p>
            <p class="muted">To cope with these challenges, a simple modification of standard fine-tuning is employed: Two-stage fine-tuning. In Stage 1, the final layer of the pretrained model is pre-finetuned. The pre-finetuning can be done either by training the last layer with class-balanced augmented data, generated using ChatGPT, or with a class-balanced reweighting method used with the original data. In Stage 2, the standard fine-tuning is performed.</p>
            <img src="images/two-stage.jpg" alt="Two-stage fine-tuning" style="max-width:60%; height:auto; margin:8px 0;">
            <p class="muted">This modification has several benefits:
            <ol>
              <li class="muted">It leverages pretrained representations by only finetuning a small portion of the model parameters while keeping the rest untouched</li>
              <li class="muted">It allows the model to learn an initial representation of the specific task</li>
              <li class="muted">It protects the learning of tail classes from being at a disadvantage during the model updating</li>
            </ol>
            </p>
            <p class="muted">The experimental results show that the proposed two-stage fine-tuning outperforms vanilla fine-tuning and state-of-the-art methods on different datasets.</p>
          </article>

          <article class="project">
            <h3>
              <a href="https://www.sciencedirect.com/science/article/pii/S1874490718302350" target="_blank" rel="noreferrer noopener">
                Mathematical analysis and improvement of IS-LDPC codes
              </a>
            </h3>  
            <p class="muted">Peak-to-Average Power Ratio (PAPR) is a major shortcoming of Orthogonal Frequency Division Multiplexing (OFDM) systems. One promising solution to this issue is the use of Invertible-Subset Low-Density Parity-Check (IS-LDPC) codes. Although IS-LDPC codes are particularly effective at controlling PAPR with low search complexity, their error-control performance degrades as the number of invertible subsets increases.</p>
            <p class="muted">To investigate the reasons for this performance degradation, I conducted a mathematical analysis of the code construction and proved four theorems that determine the probability of key events in the construction of such codes, while also establishing bounds on the decisions that influence their performance. Based on these theorems, I proposed a heuristic search method designed to enhance the error-control performance of IS-LDPC codes, while maintaining their favorable PAPR control characteristics and low search complexity. Computer simulations show that the proposed method decreases the bit error rate (BER) and frame error rate (FER) across different configurations of IS-LDPC codes.</p>
            <img src="images/IS-LDPC.png" alt="Mathematical analysis of construction of IS-LDPC codes" style="max-width:60%; height:auto; margin:8px 0;">
          </article>

          <article class="project">
            <h3>
              <a href="https://arxiv.org/abs/2403.20284" target="_blank" rel="noreferrer noopener">
                LayerNorm: A key component in parameter-efficient fine-tuning
              </a>
            </h3>  
            <p class="muted">Transformer-based models achieve strong performance across many NLP tasks. However, due to their large number of parameters, these models are computationally expensive to fine-tune for downstream tasks. This cost grows as the number of tasks increases. Furthermore, the combination of many parameters and limited labeled data for a given task can lead to overfitting and poor generalization on out-of-distribution data. A popular approach to mitigate this issue is to fine-tune only a small subset of the model parameters rather than performing full fine-tuning.</p>
            <p class="muted">To find a computationally efficient find tuning method, I first examine how different components of BERT change during full fine-tuning and identify LayerNorm as a key component. I show that LayerNorm has the highest Fisher information among all components of BERT. This is consistent with findings from other studies, which show that—unlike other components—the performance of BERT-family models degrades significantly when LayerNorm is disabled. This further underscores the critical role of this component.  Based on these observations, I perform parameter-efficient fine-tuning by training only the LayerNorm component. My results demonstrate that fine-tuning only LayerNorm achieves comparable performance to another state-of-the-art parameter-efficient tuning (bias-only tuning), while requiring just one-fifth the number of parameters. Moreover, I show that strong performance can still be obtained when only a portion of LayerNorm is trained. Importantly, this portion can be selected based on information from the target downstream task or even from other tasks, without sacrificing performance. Achieving strong results with subsets of LayerNorm chosen from unrelated tasks highlights that these subsets can be optimized in a problem-agnostic manner.</p>
            <!-- <img src="images/IS-LDPC.png" alt="LayerNorm: A key component in parameter-efficient fine-tuning" style="max-width:60%; height:auto; margin:8px 0;"> -->
          </article>

        </div>
      </section>



      
      <section id="Experience">
        <h2>Experience</h2>
        <ul class="edu-list">
          <li>
            <strong>Comcast</strong> — Software Engineer<br>
            <span class="muted">Philadelphia, PA • Sept 2023 – Present</span>
          </li>
          <li>
            <strong>Drexel University</strong> — Graduate Research and Teaching Assistant<br>
            <span class="muted">Philadelphia, PA • Sept 2018 – Sept 2023</span>
          </li>
        </ul>
      </section>      

      

      <section id="Education">
        <h2>Education</h2>
        <ul class="edu-list">
          <li>
            <strong>Drexel University</strong> — Ph.D. in Electrical Engineering<br>
            <span class="muted">Philadelphia, PA • Sept 2018 – Mar 2024</span>
          </li>
          <li>
            <strong>Iran University of Science and Technology</strong> — M.Sc. in Electrical Engineering<br>
            <span class="muted">Tehran, Iran • Sept 2014 – Jan 2017</span>
          </li>
          <li>
            <strong>Lorestan University</strong> — B.Sc. in Electrical Engineering<br>
            <span class="muted">Khorramabad, Iran • Feb 2009 – May 2013</span>
          </li>
        </ul>
      </section>

      

      <section id="Publications">
        <h2>Publications</h2>
      
        <h3>Journal and Conference Papers</h3>
        <ul class="pub-list">
          <li>
            <a href="https://doi.org/10.1016/j.neucom.2024.127801">
              <strong>Taha ValizadehAslani</strong>, Yiwen Shi, Jing Wang, Ping Ren, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang.
              “Two-Stage Fine-Tuning with ChatGPT Data Augmentation for Learning Class-Imbalanced Data.”
              <em>Neurocomputing</em>, 127801. 2024.
            </a>
          </li>
      
          <li>
            <a href="https://arxiv.org/abs/2403.20284">
              <strong>Taha ValizadehAslani</strong>, Hualou Liang.
              “LayerNorm: A key component in parameter-efficient fine-tuning.” arXiv:2403.20284.
            </a>
          </li>
      
          <li>
            <a href="https://academic.oup.com/bib/advance-article-abstract/doi/10.1093/bib/bbad226/7197744?redirectedFrom=fulltext">
              <strong>Taha ValizadehAslani</strong>, Yiwen Shi, Ping Ren, Jing Wang, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang.
              “PharmBERT: a domain-specific BERT model for drug labels.”
              <em>Briefings in Bioinformatics</em> 24(4), bbad226. 2023.
            </a>
          </li>
      
          <li>
            <a href="https://doi.org/10.1016/j.jbi.2023.104533">
              Yiwen Shi, Ping Ren, Jing Wang, Biao Han, <strong>Taha ValizadehAslani</strong>, Felix Agbavor, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang.
              “Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting.”
              <em>Journal of Biomedical Informatics</em> 148, 104533. 2023.
            </a>
          </li>
      
          <li>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S1532046423000060?CMX_ID=&SIS_ID=&dgcid=STMJ_AUTH_SERV_PUBLISHED&utm_acid=88471714&utm_campaign=STMJ_AUTH_SERV_PUBLISHED&utm_in=DM331526&utm_medium=email&utm_source=AC_">
              Yiwen Shi, Jing Wang, Ping Ren, <strong>Taha ValizadehAslani</strong>, Yi Zhang, Meng Hu, Hualou Liang.
              “Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug Labeling to Enhance Product-Specific Guidance Assessment.”
              <em>Journal of Biomedical Informatics</em> 138, 104285. 2023.
            </a>
          </li>
      
          <li>
            <a href="https://proceedings.mlr.press/v183/shi22a.html">
              Yiwen Shi, <strong>Taha ValizadehAslani</strong>, Jing Wang, Ping Ren, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang.
              “Improving Imbalanced Learning by Pre-finetuning with Data Augmentation.”
              <em>Fourth International Workshop on Learning with Imbalanced Domains: Theory and Applications</em>. 2022.
            </a>
          </li>
      
          <li>
            <a href="https://www.frontiersin.org/articles/10.3389/fgene.2021.628758/full">
              Waleed Iqbal, Elena V. Demidova, Samantha Serrao, <strong>Taha ValizadehAslani</strong>, Gail Rosen, Sanjeevani Arora.
              “RRM2B Is Frequently Amplified Across Multiple Tumor Types: Implications for DNA Repair, Cellular Survival, and Cancer Therapy.”
              <em>Frontiers in Genetics</em> 12, 628758. 2021.
            </a>
          </li>
      
          <li>
            <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7694136/">
              <strong>Taha ValizadehAslani</strong>, Zhengqiao Zhao, Bahrad A Sokhansanj, Gail L Rosen.
              “Amino Acid k-mer Feature Extraction for Quantitative Antimicrobial Resistance (AMR) Prediction by Machine Learning and Model Interpretation for Biological Insights.”
              <em>Biology</em> 9(11), 365. 2020.
            </a>
          </li>
      
          <li>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7583716/">
              Chiahui Chen, <strong>Taha ValizadehAslani</strong>, Gail Rosen, Carla Jungquist, Laura Anderson.
              “Healthcare Shift Workers’ Temporal Habits for Eating, Sleeping, and Light Exposure: A Multi-Instrument Pilot Study.”
              <em>Journal of Circadian Rhythms</em> 18. 2020.
            </a>
          </li>
      
          <li>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S1874490718302350">
              <strong>Taha ValizadehAslani</strong>, Abolfazl Falahati.
              “An analysis and improvement of error control performance of IS-LDPC codes with a large number of subsets.”
              <em>Physical Communication</em> 31, 79–86. 2018.
            </a>
          </li>
        </ul>
      
        <h3>Academic Peer Review</h3>
        <ul class="pub-list">
          <li>Reviewed for <em>PLOS Digital Health</em>.</li>
        </ul>
      </section>
            


      <section id="books">
        <h2>Book Recommendations</h2>
        Here are books I’ve read and recommend, loosely classified by topic.

        <h3>Psychology and Social Sciences</h3>
          <ul class="book-list">  
            <li><strong>The Righteous Mind</strong> by Jonathan Haidt</li>
            <li><strong>The Coddling of the American Mind</strong> by Greg Lukianoff and Jonathan Haidt<br></li>
            <li><strong>Is There Anything Good About Men?</strong> by Roy F. Baumeister<br></li>
            <li><strong>Dopamine Nation</strong> by Anna Lembke<br></li>
            <li><strong>Social Justice Fallacies</strong> by Thomas Sowell<br></li>
            <li><strong>Introducing Feminism: A Graphic Guide</strong> by Cathia Jenainati and Judy Groves<br></li>
            <li><strong>Introducing Slavoj Žižek: A Graphic Guide</strong> by Christopher Kul-Want and Piero Pierini<br></li>
            <li><strong>The ADHD Explosion</strong> by Richard Scheffler and Stephen P. Hinshaw<br></li>
         </ul>

        <h3>Evolution</h3>
          <ul class="book-list">  
            <li><strong>Grooming, Gossip and the Evolution of Language</strong> by Robin Dunbar</li>
            <li><strong>Catching Fire</strong> by Richard Wrangham</li>
            <li><strong>The Deep History of Ourselves</strong> by Joseph E. LeDoux</li>
            <li><strong>The Mating Mind</strong> by Geoffrey Miller</li>            
            <li><strong>Guns, Germs, and Steel</strong> by Jared M. Diamond</li>
            <li><strong>Survival of the Prettiest</strong> by Nancy Etcoff</li>
         </ul>        
        
        <h3>Philosophy</h3>
          <ul class="book-list">  
            <li><strong>A History of Western Philosophy</strong> by Bertrand Russell</li>
            <li><strong>A History of Philosophy</strong> by Frederick Charles Copleston</li>
            <li><strong>The Story of Philosophy</strong> by Will Durant</li>
            <li><strong>Sophie's World</strong> by Jostein Gaarder</li>
            <li><strong>Meditations</strong> by Marcus Aurelius</li>
            <li><strong>The Natural History of Religion</strong> by David Hume</li>
         </ul>        

        <h3>Economics and Politics</h3>
          <ul class="book-list"> 
            <li><strong>Economic & Philosophic Manuscripts of 1844</strong> by Karl Marx<br></li>
            <li><strong>The Signal and the Noise</strong> by Nate Silver</li>
            <li><strong>The Climate Casino</strong> by William Nordhaus</li>
         </ul>   
        
        <h3>Physics</h3>
          <ul class="book-list">  
            <li><strong>Black Holes</strong> by Brian Cox and Jeff Forshaw</li>
            <li><strong>The Little Book of Black Holes</strong> by Steven S. Gubser and Frans Pretorius</li>
            <li><strong>The Quantum Universe</strong> by Brian Cox and Jeff Forshaw</li>
            <li><strong>Galaxy</strong> by James Geach</li>
         </ul>  

        <h3>Literature & Fiction</h3>
          <ul class="book-list">  
            <li><strong>The Castle</strong> by Franz Kafka</li>
            <li><strong>The Trial</strong> by Franz Kafka</li>
            <li><strong>The Metamorphosis</strong> by Franz Kafka</li>
            <li><strong>The Plague</strong> by Albert Camus</li>
            <li><strong>The Fall</strong> by Albert Camus</li>
            <li><strong>The Stranger</strong> by Albert Camus</li>
            <li><strong>The First Man</strong> by Albert Camus</li>
            <li><strong>Flypaper</strong> by Robert Musil</li>
            <li><strong>Blindness</strong> by José Saramago</li>
            <li><strong>The Sound and the Fury</strong> by William Faulkner</li>
            <li><strong>As I Lay Dying</strong> by William Faulkner</li>
            <li><strong>A Madman for a Hundred Liras</strong> by Aziz Nesin</li>
            <li><strong>Germinal</strong> by Émile Zola</li>
            <li><strong>Animal Farm</strong> by George Orwell</li>         
            <li><strong>The Blind Owl</strong> by Sadegh Hedayat</li>
            <li><strong>Three Drops of Blood</strong> by Sadegh Hedayat</li>
            <li><strong>The Little Prince</strong> by Antoine de Saint-Exupéry</li>
            <li><strong>Anna Karenina</strong> by Leo Tolstoy</li>
         </ul>  
        
      </section>

      
      
      <section id="contact">
        <h2>Contact</h2>
        <dl class="contact">
          <!-- <dt>Email</dt> -->
          <dd><a href="mailto:taha.valizadeh@gmail.com">taha.valizadeh@gmail.com</a></dd>
        </dl>
        <footer>
          © <span id="y"></span> Taha ValizadehAslani
        </footer>
      </section>
    </main>
  </div>
  <script>
    document.getElementById('y').textContent = new Date().getFullYear();
  </script>
</body>
</html>
